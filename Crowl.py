from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import json
import re
import time
import os

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø±Ø§ÛŒÙˆØ±
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø³ÛŒØ± Ù†Ø³Ø¨ÛŒ Ø¨Ø±Ø§ÛŒ chromedriver.exe
script_dir = os.path.dirname(__file__)
chromedriver_path = os.path.join(script_dir, 'chromedriver-win64', 'chromedriver.exe')
service = Service(chromedriver_path)
options = webdriver.ChromeOptions()
# Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ù‡ØªØ± Ø®Ø·Ø§Ù‡Ø§ headless Ø±Ùˆ Ù…ÙˆÙ‚ØªØ§ ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ù†
options.add_argument("--headless=new")
driver = webdriver.Chrome(service=service, options=options)

def extract_keywords(text):
    stop_words = {"ØªÙˆØ±", "Ù‡ØªÙ„", "Ù¾Ø±ÙˆØ§Ø²", "Ø¬Ø²Ø¦ÛŒØ§Øª", "Ù…Ø´Ø§Ù‡Ø¯Ù‡", "Ø±Ø²Ø±Ùˆ", "Ø³ÙØ±", "Ø¨Ù‡", "Ø§Ø²"}
    words = re.findall(r'\b[\w\-]+\b', text.lower())
    return [word for word in words if word not in stop_words and len(word) > 2][:5]

def extract_price(card):
    try:
        price_container = card.find_element(By.CLASS_NAME, "result__item-price")
        price_num = price_container.find_element(By.CLASS_NAME, "fa-num").text.strip()
        price_unit = price_container.find_element(By.CSS_SELECTOR, ".result__item-price-items label").text.strip()
        return f"{price_num} {price_unit}"
    except Exception as e:
        print(f"Error extracting price: {e}")
        return "Ù‚ÛŒÙ…Øª Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª"

def scrape_items_from_current_page(category):
    items = []
    cards = driver.find_elements(By.CLASS_NAME, "result__item")

    for i, card in enumerate(cards):
        try:
            title = card.find_element(By.CLASS_NAME, "result__item-header-title").text.strip()
            location = card.find_element(By.CLASS_NAME, "result__item-location").text.strip() if card.find_elements(By.CLASS_NAME, "result__item-location") else ""
            link = card.get_attribute("href") or ""

            # Extract flight, duration, and dates from result__item-header-detail
            flight = ""
            duration_text = ""
            header_details = card.find_elements(By.CLASS_NAME, "result__item-header-detail")
            if header_details:
                detail_list_items = header_details[0].find_elements(By.TAG_NAME, "li")
                for li in detail_list_items:
                    text_content = li.text.strip()
                    
                    # Check for flight info (contains airline logo or specific text)
                    if li.find_elements(By.TAG_NAME, "img"):
                        flight = text_content
                    # Check for duration (contains moon icon)
                    elif li.find_elements(By.CLASS_NAME, "icon-moon"):
                        duration_text = text_content
                    # Check for dates (contains date icon)
                    elif li.find_elements(By.CLASS_NAME, "icon-date"):
                        dates_text = text_content
                        duration_text += f" -\n{dates_text}"
            if not flight:
                flight = "Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡"
            if category == "ØªÙˆØ±Ù‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ":
                keywords_text = f"{title}"
            else:
                keywords_text = f"{title} {location}"
                
            price = extract_price(card)

            items.append({
                "question": title,
                "answer": f"<a href='{link}' target='_blank'>Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª</a>",
                "keywords": extract_keywords(keywords_text),
                "category": category,
                "flight": flight,
                "location": location,
                "duration": duration_text,
                "price": price              
            })
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢ÛŒØªÙ… {i+1} Ø¯Ø± Ø¯Ø³ØªÙ‡ {category}: {e}")
            # Optionally, print the card's outer HTML for debugging
            # print(f"HTML Ú©Ø§Ø±Øª Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±: {card.get_attribute('outerHTML')}")
    return items

def scrape_all_pages(category, initial_url, item_class_name="result__item", pagination_xpath="//ul[contains(@class, 'paging')]//a[text()='{page_num}']"):
    all_items = []
    current_page_num = 1
    
    try:
        driver.get(initial_url)
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CLASS_NAME, item_class_name))
        )
        time.sleep(2) # Give some time for dynamic content to load
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ Ø§ÙˆÙ„ÛŒÙ‡ {category} Ø¯Ø± {initial_url} (Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³: {item_class_name}): {e}")
        return []

    while True:
        print(f"ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ØµÙØ­Ù‡ {current_page_num} Ø¨Ø±Ø§ÛŒ {category}...")
        all_items.extend(scrape_items_from_current_page(category))

        try:
            # Find the next page button using the provided pagination_xpath
            next_page_xpath = pagination_xpath.format(page_num=current_page_num + 1)
            next_page_link = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, next_page_xpath))
            )
            
            # Scroll to the button and click it
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_page_link)
            time.sleep(1) # Wait for scroll to complete
            driver.execute_script("arguments[0].click();", next_page_link)
            
            # Wait for the new page to load
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, item_class_name))
            )
            time.sleep(2) # Give some time for dynamic content to load
            current_page_num += 1
        except Exception as e:
            print(f"â„¹ï¸ ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ {category} (XPath: {pagination_xpath.format(page_num=current_page_num + 1)}) ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}")
            break # No more pages or an error occurred

    return all_items

def scrape_single_article_page(article_url):
    # This function is no longer needed as we are extracting directly from the listing page.
    # Keeping it as a placeholder or for potential future use if detailed scraping is re-enabled.
    pass

# def scrape_hotels(category, url):
#     driver.get(url)
#     WebDriverWait(driver, 10).until(
#         EC.presence_of_element_located((By.CLASS_NAME, "js-best-package-select"))
#     )
#     time.sleep(1)

#     hotels = []

#     # Ù„ÛŒØ³Øª Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ select (Ù…Ø«Ù„Ø§Ù‹ Ø§Ø³ØªØ§Ù†Ø¨ÙˆÙ„ØŒ Ø¯Ø¨ÛŒ)
#     select_element = driver.find_element(By.CLASS_NAME, "js-best-package-select")
#     options = select_element.find_elements(By.TAG_NAME, "option")

#     for option in options:
#         value = option.get_attribute("value")  # Ù…Ø«Ù„Ø§Ù‹ "tab-1"
#         city = option.text.strip()             # Ù…Ø«Ù„Ø§Ù‹ "Ø§Ø³ØªØ§Ù†Ø¨ÙˆÙ„"

#         # Ø§Ù†ØªØ®Ø§Ø¨ Ú¯Ø²ÛŒÙ†Ù‡
#         driver.execute_script("arguments[0].value = arguments[1]; arguments[0].dispatchEvent(new Event('change'))", select_element, value)
#         time.sleep(1)

#         # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ Ø§ÛŒÙ† ØªØ¨
#         tab_div = driver.find_element(By.ID, value)
#         hotel_cards = tab_div.find_elements(By.CLASS_NAME, "best-package__item")

#         for card in hotel_cards:
#             try:
#                 name = card.find_element(By.CLASS_NAME, "best-package__item-title").text.strip()
#                 score = card.find_element(By.CLASS_NAME, "best-package__item-rate").text.strip()
#                 img = card.find_element(By.CSS_SELECTOR, "picture source").get_attribute("data-srcset")
#                 alt = card.find_element(By.CSS_SELECTOR, "picture img").get_attribute("alt")

#                 hotels.append({
#                     "question": f"Ù‡ØªÙ„ {name} Ø¯Ø± {city}",
#                     "answer": f"<img src='{img}' alt='{alt}' width='150'><br>Ø§Ù…ØªÛŒØ§Ø²: {score}",
#                     "keywords": extract_keywords(f"{name} {city} {score}"),
#                     "category": category,
#                     "location": city,
#                     "score": f"Ø§Ù…ØªÛŒØ§Ø² {score}"
#                 })

#             except Exception as e:
#                 print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ØªÙ„:", e)

#     return hotels

def crawl_blog_articles():
    print("\nğŸš€ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ù„Ø§Ú¯")
    blog_url = "https://www.atitravel.ir/blog/"
    articles = []

    try:
        driver.get(blog_url)
        # Wait for the main blog list container to be present
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "main.blog-list__main"))
        )
        time.sleep(2)
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡ Ø¨Ù„Ø§Ú¯ {blog_url} (Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ CSS_SELECTOR: main.blog-list__main): {e}")
        return []

    current_page_num = 1
    while True:
        print(f"ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø² ØµÙØ­Ù‡ {current_page_num} Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ù„Ø§Ú¯...")
        
        # Find the main blog list section
        try:
            blog_main_section = driver.find_element(By.CSS_SELECTOR, "main.blog-list__main")
            # Use 'latest-topic__item' for the main list of articles within this section
            article_cards = blog_main_section.find_elements(By.CLASS_NAME, "latest-topic__item")
            for card in article_cards:
                try:
                    # Extract link and title directly from the card
                    link_element = card.find_element(By.CSS_SELECTOR, "a.latest-topic__item-title")
                    link = link_element.get_attribute("href")
                    full_title = link_element.text.strip()
                    
                    country_name = "Ù†Ø§Ù…Ø´Ø®Øµ"
                    country_match = re.search(r'Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³ÙØ± Ø¨Ù‡ (.+)', full_title)
                    if country_match:
                        country_name = country_match.group(1).strip()
                    else:
                        # Fallback if title doesn't match pattern
                        # Try to get from alt attribute of image or other text
                        try:
                            img_alt = card.find_element(By.TAG_NAME, "img").get_attribute("alt")
                            alt_match = re.search(r'Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³ÙØ± Ø¨Ù‡ (.+)', img_alt)
                            if alt_match:
                                country_name = alt_match.group(1).strip()
                            else:
                                country_name = full_title.split()[-1] # Last word as a fallback
                        except:
                            country_name = full_title.split()[-1] # Last word as a fallback

                    if link and link not in [art['link'] for art in articles]: # Avoid re-processing already added articles
                        articles.append({
                            "question": f"Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³ÙØ± Ø¨Ù‡ {country_name}",
                            "answer": f"<a href='{link}' target='_blank'>Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…Ù‚Ø§Ù„Ù‡ {country_name}</a>",
                            "keywords": [country_name] if country_name != "Ù†Ø§Ù…Ø´Ø®Øµ" else [],
                            "category": "Ù…Ù‚Ø§Ù„Ø§Øª",
                            "link": link
                        })
                        print(f"   âœ… Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: Ø¹Ù†ÙˆØ§Ù†='{country_name}', Ù„ÛŒÙ†Ú©='{link}'")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© ÛŒØ§ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² Ø¢ÛŒØªÙ… 'latest-topic__item' Ø¯Ø± ØµÙØ­Ù‡ {current_page_num}: {e}")
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ÛŒØ§ÙØªÙ† Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ø¨Ù„Ø§Ú¯ (main.blog-list__main) Ø¯Ø± ØµÙØ­Ù‡ {current_page_num}: {e}")

        # Also check for 'topic-card' elements in the "Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…" section (if it exists outside the main list)
        featured_article_cards = driver.find_elements(By.CLASS_NAME, "topic-card")
        for card in featured_article_cards:
            try:
                link_element = card.find_element(By.TAG_NAME, "a")
                link = link_element.get_attribute("href")
                full_title = link_element.get_attribute("title") or link_element.text.strip()

                country_name = "Ù†Ø§Ù…Ø´Ø®Øµ"
                country_match = re.search(r'Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³ÙØ± Ø¨Ù‡ (.+)', full_title)
                if country_match:
                    country_name = country_match.group(1).strip()
                else:
                    country_name = full_title.split()[-1] # Last word as a fallback

                if link and link not in [art['link'] for art in articles]:
                    articles.append({
                        "question": f"Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³ÙØ± Ø¨Ù‡ {country_name}",
                        "answer": f"<a href='{link}' target='_blank'>Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…Ù‚Ø§Ù„Ù‡ {country_name}</a>",
                        "keywords": [country_name] if country_name != "Ù†Ø§Ù…Ø´Ø®Øµ" else [],
                        "category": "Ù…Ù‚Ø§Ù„Ø§Øª",
                        "link": link
                    })
                    print(f"   âœ… Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: Ø¹Ù†ÙˆØ§Ù†='{country_name}', Ù„ÛŒÙ†Ú©='{link}' (Ø§Ø² topic-card)")
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© ÛŒØ§ Ø¹Ù†ÙˆØ§Ù† Ø§Ø² Ø¢ÛŒØªÙ… 'topic-card' Ø¯Ø± ØµÙØ­Ù‡ {current_page_num}: {e}")

        if not articles: # Check if any articles were found on this page
            print(f"âš ï¸ Ù‡ÛŒÚ† Ù„ÛŒÙ†Ú© Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø¯Ø± ØµÙØ­Ù‡ {current_page_num} ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù¾Ø§ÛŒØ§Ù† Ù¾ÛŒÙ…Ø§ÛŒØ´ Ø¨Ù„Ø§Ú¯.")
            break
        
        # Return to the blog listing page to find the next pagination link
        driver.get(blog_url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "main.blog-list__main"))
        )
        time.sleep(2)

        try:
            # The pagination class is 'pagination' for blog
            next_page_xpath = f"//ul[contains(@class, 'pagination')]//a[text()='{current_page_num + 1}']"
            next_page_link = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, next_page_xpath))
            )
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_page_link)
            time.sleep(1)
            driver.execute_script("arguments[0].click();", next_page_link)
            
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "main.blog-list__main"))
            )
            time.sleep(2)
            current_page_num += 1
        except Exception as e:
            print(f"â„¹ï¸ ØµÙØ­Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ù„Ø§Ú¯ (XPath: {next_page_xpath}) ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {e}")
            break
    return articles

def crawl_tours():
    base_url = "https://www.atitravel.ir"
    endpoints = {
        "ØªÙˆØ±Ù‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ": "/tours/external/",
        "ØªÙˆØ±Ù‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ": "/tours/internal/",
        # "Ù‡ØªÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ": "/externalhotel/",
        # "Ù‡ØªÙ„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ": "/internalhotel/",
    }

    faqs = {}

    for category, endpoint in endpoints.items():
        url = base_url + endpoint
        # Use specific pagination XPATH for hotels if different, otherwise default 'paging'
        # Assuming hotels also use 'paging' for now, but can be adjusted if needed.
        try:    
            if "Ù‡ØªÙ„" in category:
                pass
                # faqs[category] = scrape_hotels(category, url)
            else :
                faqs[category] = scrape_all_pages(category, url, "result__item")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {category}: {e}")
            faqs[category] = []
    try:        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ù„Ø§Ú¯
        faqs["Ù…Ù‚Ø§Ù„Ø§Øª"] = crawl_blog_articles()
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø§Ù„Ø§Øª Ø¨Ù„Ø§Ú¯: {e}")

    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
    with open('ati.json', 'w', encoding='utf-8') as f:
        json.dump({"faqs": faqs, "metadata": {"last_updated": time.strftime("%Y-%m-%d")}}, f, ensure_ascii=False, indent=4)

    driver.quit()
    print("âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø´Ø¯ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")

# Ø§Ø¬Ø±Ø§
if __name__ == "__main__":
    crawl_tours()
